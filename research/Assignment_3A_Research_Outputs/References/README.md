# References - CyberPrompt Research Project

**Research Project**: Benchmarking Generative AI Token Use in Cybersecurity Operations  
**Student**: Mohamed Zeyada (11693860)  
**Date**: October 2025

---

## Bibliography Overview

This directory contains the complete academic bibliography used in the CyberPrompt research project. All references have been verified for academic credibility and relevance to the research objectives.

### Reference Categories
- **Prompt Engineering Research**: 3 references
- **Cybersecurity AI Applications**: 4 references  
- **Benchmarking & Evaluation**: 4 references
- **Performance Measurement**: 3 references

### Total References: 14 peer-reviewed academic sources

---

## File Contents

### `ACADEMIC_BIBLIOGRAPHY_UPDATED.md`
Updated formatted bibliography with:
- APA-style citations
- DOI links where available
- Publication details and journal information
- Four new cybersecurity research papers
- Removal of less relevant references
- Reorganized sections for better clarity

### Key Additions
1. **CySecBench (2025)**: Cybersecurity-focused prompt dataset for benchmarking LLMs
2. **DefenderBench (2025)**: Toolkit for evaluating language agents in cybersecurity environments
3. **Same Evaluation, More Tokens (2025)**: Analysis of input length effects on LLM evaluation
4. **Generative Benchmarking (2025)**: Methodology for creating representative test datasets

### Usage in Research
These references support:
1. **Literature Review Section**: Theoretical foundation for prompt optimization
2. **Methodology Development**: Quality assessment framework design
3. **Experimental Design**: Controlled experiment methodology
4. **Results Analysis**: Statistical analysis and evaluation approaches

---

*References updated for Assignment 3A Research Outputs Portfolio*
