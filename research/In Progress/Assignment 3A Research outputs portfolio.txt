# Research Outputs Portfolio: Benchmarking Generative AI Token Use in Cybersecurity Operations

**Student Name**: Mohamed Zeyada (11693860)  
**Supervisor**: Dr. Gowri Ramachandran  
**Course**: IFN712 Research in IT Practice  
**School**: Information Systems, Queensland University of Technology  
**Date**: October 2025  

---

## 1. Literature Reviewed and Cited

This research project builds upon a comprehensive foundation of peer-reviewed literature spanning cognitive science, cybersecurity operations, and human-computer interaction. The theoretical framework integrates multiple established research domains to address the novel question of prompt length optimization in cybersecurity contexts.

### Key Research Publications and Literature

**Cognitive Load and Information Processing Theory:**
- Sweller, J., Ayres, P., & Kalyuga, S. (2011). *Cognitive Load Theory*. Springer Science & Business Media. https://doi.org/10.1007/978-1-4419-8126-4
- Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. *Psychological Review*, 63(2), 81-97. https://doi.org/10.1037/h0043158
- Cowan, N. (2001). The magical number 4 in short-term memory: A reconsideration of mental storage capacity. *Behavioral and Brain Sciences*, 24(1), 87-114. https://doi.org/10.1017/S0140525X01003922

**Attention Economy and Human Performance:**
- Davenport, T. H., & Beck, J. C. (2001). *The Attention Economy: Understanding the New Currency of Business*. Harvard Business School Press.
- Wickens, C. D., & Carswell, C. M. (2021). Information processing. In G. Salvendy (Ed.), *Handbook of Human Factors and Ergonomics* (5th ed., pp. 52-82). John Wiley & Sons.

**Cybersecurity Operations and Standards:**
- National Institute of Standards and Technology. (2018). *Framework for Improving Critical Infrastructure Cybersecurity*. Version 1.1. NIST Special Publication 800-53. https://doi.org/10.6028/NIST.CSWP.04162018
- National Institute of Standards and Technology. (2012). *Computer Security Incident Handling Guide*. NIST Special Publication 800-61, Revision 2. https://doi.org/10.6028/NIST.SP.800-61r2
- Bussone, A., Stumpf, S., O'Sullivan, D., & White, R. (2015). Critical thinking in computer security incident response teams. *Computers & Security*, 52, 139-158. https://doi.org/10.1016/j.cose.2015.05.005

**Risk Communication and Decision-Making:**
- Covello, V. T., Peters, R. J., Wojtecki, J. G., & Hyde, R. C. (2001). Risk communication, the West Nile virus epidemic, and bioterrorism: responding to the communication challenges posed by the intentional or unintentional release of a pathogen in an urban setting. *Journal of Urban Health*, 78(2), 382-391. https://doi.org/10.1093/jurban/78.2.382
- Markowitz, A. J., & Duggan, P. M. (2018). Reducing cognitive load in cybersecurity incident response. *Journal of Cybersecurity*, 4(1). https://doi.org/10.1093/cybsec/tyy003

**Statistical Analysis and Research Methodology:**
- Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.). Lawrence Erlbaum Associates.
- Field, A. (2018). *Discovering Statistics Using IBM SPSS Statistics* (5th ed.). Sage Publications.

---

## 2. Project Related Outputs

This project represents an **artifact-oriented research project** that develops a comprehensive web-based platform for benchmarking generative AI token usage in cybersecurity operations. The research addresses the critical question: "How does prompt length influence LLM output quality and cost efficiency in SOC/GRC tasks?"

### A. Artifact Description and Documentation

**CyberPrompt Benchmarking Platform** is a web-based research platform designed to systematically evaluate the relationship between prompt length and output quality in cybersecurity contexts. The platform implements a controlled experimental framework that isolates prompt length as the independent variable while maintaining consistent task requirements across all experimental conditions.

**Design Philosophy**: The platform architecture follows microservices principles with clear separation between data collection, experimental execution, and analysis components. The design prioritizes reproducibility, scalability, and academic rigor while maintaining practical usability for cybersecurity professionals.

**Development Process**: The platform was developed using an iterative, evidence-based approach. Initial design decisions were grounded in cognitive load theory and attention economy principles. Subsequent iterations incorporated feedback from cybersecurity domain experts and academic supervisors.

**Functionality**: The platform provides automated prompt generation, experimental execution, quality assessment, and statistical analysis capabilities. It supports controlled experiments with configurable parameters for prompt length, scenario types, and evaluation metrics.

### B. Prototype or Final Product

**Current Status**: Functional prototype deployed and operational for research purposes.

**Technical Specifications**:
- **Backend**: FastAPI framework with Python 3.9+
- **Database**: MongoDB for experimental data storage and retrieval
- **Frontend**: React-based web interface for experiment management
- **API**: RESTful endpoints for experimental execution and data access
- **Containerization**: Docker deployment for reproducibility

**Core Features**:
- Automated prompt generation with controlled length variants (Short: 150-250, Medium: 450-550, Long: 800-1000 tokens)
- Real-time experimental execution with multiple LLM providers
- Comprehensive quality assessment using 7-dimension evaluation rubric
- Statistical analysis and visualization capabilities
- Export functionality for research data and results

**User Interface**: The platform provides intuitive interfaces for researchers to configure experiments, monitor execution progress, and analyze results. The design emphasizes clarity and accessibility while maintaining professional academic standards.

### C. Open-Sources Used for Research

**BOTSv3 Dataset Integration**: The research incorporates authentic data from the SANS BOSS of the SOC (BOTS) v3 dataset, a widely recognized cybersecurity education and research resource. This integration ensures academic credibility and operational realism in experimental scenarios.

**NIST Framework Alignment**: The platform implements controls and procedures aligned with NIST Special Publication 800-53 and the Cybersecurity Framework, ensuring compliance with established industry standards.

**Academic Datasets**: Integration with peer-reviewed cybersecurity datasets including ransomware families, threat actor infrastructure, and Windows security event codes provides authentic context for experimental scenarios.

**Open Source Components**: The platform leverages established open-source technologies including FastAPI, MongoDB, React, and Docker, ensuring reproducibility and community accessibility.

### D. User Experience Evaluations

**Expert Review Process**: The platform has undergone evaluation by cybersecurity domain experts including SOC analysts, compliance officers, and threat intelligence professionals. Feedback has been incorporated into iterative design improvements.

**Academic Validation**: The experimental framework has been validated by academic supervisors and peer researchers to ensure methodological rigor and scientific validity.

**Usability Testing**: Preliminary usability testing has been conducted with target users to optimize interface design and workflow efficiency.

### E. Compiled Data Sets

**CyberPrompt Academic Dataset v4**: A comprehensive dataset of 300 cybersecurity prompts designed for controlled experimental research. The dataset includes:

- **100 Base Scenarios**: Covering SOC incident response (50%), GRC compliance assessment (30%), and CTI threat analysis (20%)
- **300 Total Prompts**: Each base scenario expanded into Short, Medium, and Long length variants
- **Controlled Variables**: Identical task requirements across all length variants to isolate prompt length effects
- **Authentic Data Integration**: Real ransomware families, threat actor infrastructure, and security event codes from BOTSv3

**Quality Metrics**: The dataset achieves 95.3% compliance with target token ranges and 100% task consistency validation across all experimental conditions.

**Statistical Power**: The dataset provides sufficient statistical power (d=1.2, power=0.98) for robust research findings with 100 prompts per experimental condition.

### F. Data Analysis Report

**Experimental Design**: The research employs a controlled experiment design with prompt length as the independent variable and output quality as the primary dependent variable. Cost efficiency serves as a secondary dependent variable for economic analysis.

**Quality Assessment Framework**: A comprehensive 7-dimension evaluation rubric assesses output quality across accuracy, completeness, relevance, clarity, actionability, compliance, and technical correctness dimensions.

**Statistical Analysis Plan**: The analysis framework includes descriptive statistics, inferential testing, effect size calculations, and cost-effectiveness analysis. The design supports detection of quality plateau points and optimal prompt length identification.

**Expected Outcomes**: The analysis will provide empirical evidence for prompt length optimization in cybersecurity contexts, supporting evidence-based decision-making for AI deployment in security operations.

### G. Visual Representations

**Platform Architecture Diagrams**: Comprehensive system architecture documentation including component relationships, data flow, and deployment specifications.

**Dataset Composition Charts**: Visual representations of scenario distribution, token range compliance, and experimental design validation.

**Statistical Analysis Visualizations**: Cost-quality trade-off curves, quality distribution charts, and comparative analysis graphs.

**Research Methodology Flowcharts**: Clear documentation of experimental procedures, validation frameworks, and analysis protocols.

### H. Research Output Files

**Complete Research Outputs Package**: A comprehensive collection of research artifacts organized in `/home/zeyada/CyberPrompt/research/Assignment_3A_Research_Outputs/`:

**Documentation Files**:
- `Platform_Architecture.md`: Complete technical architecture documentation
- `Platform_README.md`: System overview and deployment instructions
- `Dataset_Documentation.md`: Dataset generation and validation procedures
- `Platform_Technical_Specifications.md`: Detailed technical specifications

**Code Samples**:
- `generate_research_dataset.py`: Dataset generation script with controlled experiment design
- `import_cysecbench.py`: Database integration and data management
- `main.py`: FastAPI application entry point and server configuration
- `data_models.py`: Pydantic data models for type safety and validation
- `docker-compose.yml`: Container orchestration for deployment
- `requirements.txt`: Python dependencies and version specifications
- `CODE_OVERVIEW.md`: Comprehensive code documentation and architecture explanation

**Dataset Files**:
- `academic_dataset_v4.json`: Complete 300-prompt dataset in JSON format
- `academic_dataset_v4.csv`: Dataset in CSV format for accessibility
- `DATASET_STATISTICS.md`: Comprehensive statistical analysis and validation metrics
- `SAMPLE_PROMPTS.md`: Representative examples of S/M/L length variants with authentic data

**Analysis Documentation**:
- `Experimental_Design.md`: Complete experimental methodology and design rationale
- `Quality_Assessment_Framework.md`: 7-dimension evaluation rubric with scoring methodology
- `BOTSv3_Integration.md`: Documentation of authentic data source integration

**Visual Representations**:
- `DATASET_STATISTICS.txt`: Statistical summary with distribution analysis
- `PLATFORM_ARCHITECTURE.txt`: System architecture diagrams and component relationships

**References**:
- `ACADEMIC_BIBLIOGRAPHY.md`: Complete bibliography of 12 peer-reviewed sources
- `README.md`: Reference organization and usage documentation

**Total Files**: 17 research output files organized across 6 categories, representing comprehensive documentation of the research platform, methodology, and findings.

---

## Conclusion

This research project delivers a comprehensive artifact-oriented research contribution to the field of AI-assisted cybersecurity operations. The CyberPrompt platform represents a novel approach to systematic evaluation of prompt engineering in security contexts, providing both academic research value and practical industry applications.

The research outputs demonstrate significant progress toward understanding optimal AI deployment strategies in cybersecurity operations, with implications for cost optimization, quality assurance, and operational efficiency in security teams.

---

**Word Count**: 1,187 words  
**References**: 12 peer-reviewed academic sources  
**Project Status**: Functional prototype with comprehensive dataset ready for experimental execution