Name Mohamed Zeyada
Student Number 11693860
Cluster 8
Supervisor Dr. Gowri Ramachandran
Major Cyber Security
IFN712 RESEARCH IN IT
PRACTICE
ASSIGNMENT 2A: RESEARCH
PROGRESS REPORT
BENCHMARKING GENERATIVE AI TOKEN USE IN
CYBERSECURITY OPERATIONS
IFN712 RESEARCH IN IT PRACTICE 1
1 Introduction
Generative Artificial Intelligence (GenAI) is transforming cybersecurity operations,
particularly within Security Operations Centers (SOC) and Governance, Risk, and
Compliance (GRC) workflows. Large Language Models (LLMs) such as GPT-4,
Claude, and Gemini are increasingly deployed for high-stakes tasks including
incident report generation, threat intelligence summarization, and compliance
documentation. While these applications enhance analyst efficiency and analytical
scope, security-critical environments demand non-negotiable standards for
accuracy, compliance adherence, and cost efficiency. SOC analysts must triage
incidents within minutes, while GRC professionals operate under strict audit
deadlines where verbose or inefficient AI outputs can compromise operational
resilience and compliance readiness.
Current cybersecurity LLM benchmarking suffers from significant fragmentation.
Some benchmarks employ SOC/GRC rubrics but lack cost analysis (CySecBench),
others offer task realism without SOC/GRC scoring (DefenderBench), and some
adapt datasets without security focus (Chroma). No unified framework integrates all
these critical components. This gap is particularly problematic as SOC and GRC
teams face tight budgets where API usage directly impacts costs, strict compliance
requirements where outputs must meet regulatory standards, and rapidly evolving
threats that static benchmarks fail to capture.
This research aims to develop the first continuous SOC/GRC benchmarking platform
that integrates cost-quality evaluation with bias mitigation and adaptive dataset
generation. The project will benchmark LLMs by linking token usage and API costs
to output quality across seven SOC/GRC-relevant dimensions while applying Focus
Sentence Prompting (FSP) to mitigate verbosity bias. Two primary research
questions guide this investigation: How does prompt length influence LLM output
quality and cost efficiency in SOC/GRC tasks? Can adaptive generative
benchmarking from SOC/GRC documents and cyber threat intelligence improve
evaluation coverage and relevance over static datasets?
This progress report documents the research advancements achieved over eight
weeks and outlines the roadmap toward developing a comprehensive, cost-aware
benchmarking framework for cybersecurity AI applications.
IFN712 RESEARCH IN IT PRACTICE 2
2 Literature Review
2.1 Introduction
This literature review examines current LLM benchmarking methodologies for
cybersecurity operations, focusing on cost-efficient evaluation frameworks for SOC/GRC
environments. The review adopts a thematic approach, organizing research into five
domains addressing the research questions: cybersecurity-specific benchmarking, token
efficiency optimization, prompt engineering effects, adaptive evaluation methodologies, and
bias mitigation strategies. The scope encompasses recent advances in LLM evaluation
(2020-2025), synthesizing findings from 12 selected papers to establish theoretical
foundations for cost-aware, adaptive benchmarking frameworks in cybersecurity
applications.
2.2 Body
Cybersecurity-Specific LLM Benchmarking Foundations
Wahréus, Hussain, and Papadimitratos (2025) introduced CySecBench, containing
12,662 prompts across ten attack categories for evaluating LLM performance in
cybersecurity operations. Their work demonstrates reproducible scoring using
domain-specific rubrics but lacks token-cost analytics and adaptive updating. Zhang
et al. (2025) developed DefenderBench, a multi-task evaluation toolkit
encompassing malicious content detection, vulnerability remediation, and security
question answering under realistic operational constraints, though without
SOC/GRC-specific rubric alignment.
Token Efficiency and Cost Optimization
Han et al. (2024) introduced the Token-Budget-Aware LLM Reasoning (TALE)
framework, achieving 68.9% token reduction with <5% accuracy loss in Chain-of-
Thought reasoning tasks, directly addressing cost efficiency concerns in multi-step
threat analysis. Enterprise research has demonstrated 4-7% cost reductions while
improving output quality. Srivastava et al. (2023) established precedent for largescale
LLM evaluation across 204 diverse tasks, demonstrating predictable scaling
behaviors across six orders of magnitude in model size.
Prompt Engineering and Length Effects
Domhan and Zhu (2025) investigated verbosity bias in LLM evaluation, introducing
Focus Sentence Prompting (FSP) to normalize prompt length and reduce linguistic
inflation bias. Their findings demonstrate that text length significantly impacts
evaluation, with longer texts leading to fewer identified error spans and reduced
system ranking accuracy. Domain-specific research confirms that longer prompts
IFN712 RESEARCH IN IT PRACTICE 3
containing relevant background knowledge improve performance across specialized
cybersecurity tasks.
Adaptive and Generative Benchmarking Methodologies
Hong et al. (2025) present comprehensive generative benchmarking methods
incorporating target documents and ground truth queries into LLM prompts,
demonstrating generation of distinct, representative queries validated through
cosine similarity distributions. Their approach addresses static benchmark
limitations by enabling continuous adaptation to new document sources. Saad-
Falcon et al. (2024) developed ARES, an automated evaluation framework providing
reference-free assessment across context relevance, answer faithfulness, and
answer relevance using fine-tuned lightweight judges.
Bias Mitigation and Fairness Frameworks
Gallegos et al. (2024) provide systematic taxonomies for bias evaluation metrics,
datasets, and mitigation techniques across different model levels, essential for
preventing biased threat detection based on superficial input characteristics.
Advanced research demonstrates inference-only methods for identifying and
eliminating biased components through attention mechanism manipulation.
2.3 Conclusion
The literature reveals significant advances in LLM evaluation while identifying
critical gaps justifying this research. Existing cybersecurity benchmarks provide
domain-specific foundations but lack integration of cost analysis, adaptive updating
mechanisms, and comprehensive bias mitigation. Current research demonstrates
token optimization feasibility and prompt length effects on performance and cost,
supporting RQ1's investigation. However, substantial gaps remain in unified
frameworks combining cost-performance analysis with domain-specific evaluation
rubrics and adaptive dataset generation. This research fills these gaps through
CyberCQBench, combining domain expertise with operational realism and advanced
bias mitigation methodologies.
3 Methodology
Brief Overview
This research adopts an artifact-oriented research methodology (Vaishnavi & Kuechler,
2013) to develop a cost-quality-aware SOC/GRC benchmarking platform for evaluating
Large Language Models in cybersecurity operations. This methodology is appropriate for
addressing our research questions as it focuses on developing and evaluating practical
IFN712 RESEARCH IN IT PRACTICE 4
solutions that cybersecurity professionals can use to optimize their LLM deployments while
maintaining compliance and operational effectiveness.
Research Design
The study employs an artifact-based research design following the Design Science
Research framework (Hevner et al., 2004). The primary artifact is an integrated
benchmarking platform that combines rubric-based evaluation, token-cost analytics, and
adaptive prompt generation. This design is appropriate because our research objectives
center on creating a functional solution that addresses the identified gap in existing
cybersecurity LLM evaluation tools.
Data Collection Methods and Materials
The platform development will integrate multiple existing datasets and frameworks:
 CySecBench dataset (Wahréus et al., 2025): 12,662 cybersecurity-focused prompts
across ten attack categories, adapted for SOC/GRC contexts
 DefenderBench task formats (Zhang et al., 2025): Multi-task evaluation scenarios
including malicious content detection and vulnerability remediation
 SOC/GRC compliance standards: NIST 800-53, ISO 27001, and GDPR frameworks
for developing the seven-dimension evaluation rubric
 Focus Sentence Prompting techniques (Domhan & Zhu, 2025): Implemented to
mitigate verbosity bias in evaluations
The artifact development will incorporate adaptive prompt generation inspired by
Chroma's methodology (Hong et al., 2025), modified to accommodate live cyber threat
intelligence feeds and evolving compliance requirements.
Data Sources and Validation
Primary data sources include established cybersecurity benchmarks and regulatory
frameworks. The seven-dimension rubric (Technical Accuracy, Actionability, Completeness,
Compliance Alignment, Risk Awareness, Relevance, Clarity) will be validated against
existing SOC/GRC evaluation criteria. Reliability will be ensured through reproducible
scoring mechanisms and standardized evaluation protocols.
Data Analysis Techniques
The platform will employ both quantitative and qualitative analysis methods:
 Quantitative analysis: Token usage tracking, cost-performance correlation
analysis, and statistical comparison of model performance across dimensions
IFN712 RESEARCH IN IT PRACTICE 5
 Qualitative analysis: Rubric-based content evaluation using the seven-dimension
framework and bias-adjusted scoring
Analysis will be implemented using Python-based evaluation tools with visualization
dashboards for cost-quality trade-off insights.
Resource Requirements and Feasibility
The project requires access to LLM APIs (GPT-4, Claude, Gemini), standard computing
resources for data processing, and existing open-source benchmarking frameworks. All
necessary resources are available through academic and open-source channels, making the
research feasible within the project timeline and budget constraints.
4 Progress to Date
Research Objectives Restatement
Our research objectives are developing a cost-quality-aware SOC/GRC
benchmarking platform addressing: (1) How does prompt length influence LLM
output quality and cost efficiency in SOC/GRC tasks? and (2) Can adaptive
generative benchmarking from SOC/GRC documents and CTI improve evaluation
coverage over static datasets?
Full-Stack Platform Implementation
We successfully developed CyberCQBench, a production-ready web application
featuring FastAPI backend with async MongoDB, React TypeScript frontend, and
Docker containerization. The platform implements our seven-dimension SOC/GRC
rubric with automated LLM judge scoring, supporting OpenAI (GPT-4, GPT-4o, GPT-
3.5-turbo) and Anthropic (Claude-3.5-Sonnet, Claude-3-Opus) models with real-time
AUD cost tracking.
Research-Grade Dataset and Systematic Analysis Framework
We created 952 research-grade prompts for systematic RQ1 analysis: 318 original
CySecBench prompts plus 317 medium (301-800 tokens) and 317 long (>800
tokens) variants across three scenarios (SOC_INCIDENT, CTI_SUMMARY,
GRC_MAPPING). This controlled S+M+L system enables precise investigation of
prompt length effects on quality and cost efficiency.
IFN712 RESEARCH IN IT PRACTICE 6
Critical Technical Breakthroughs and Challenge Resolution
We resolved an FSP integration challenge where Focus Sentence Prompting existed
in code but was disconnected from execution flow, rendering initial bias mitigation
experiments scientifically invalid. Through systematic debugging, we implemented
proper FSP integration producing measurably different evaluation patterns. We also
solved scalability issues via background task processing for large experiments (>10
runs), preventing API timeouts.
Advanced Analytics and Research Validation
The platform features comprehensive analytics including cost-efficiency frontier
analysis, statistical significance testing, KL divergence validation for RQ2,
sophisticated experiment tracking with ULID-based identification, and reproducible
scoring pipelines with fixed seeds.
Research Validation and Literature Gap Closure
CyberCQBench addresses the identified literature gap by integrating rubric
evaluation, token analytics, bias mitigation, and adaptive dataset generation in a
single framework, enabling evidence-based prompt engineering for SOC/GRC
operations with academic-grade reproducibility.
5 Expected Outputs & Future Work
5.1 Tangible Outputs
CyberCQBench Platform: Complete web-based benchmarking tool featuring
FastAPI backend, React TypeScript frontend, and MongoDB integration with Docker
Containerization. The platform implements seven-dimension SOC/GRC rubric
evaluation, real-time AUD token-cost dashboards, and Focus Sentence Prompting
bias mitigation. Complete codebase will be available via GitHub repository with
documentation.
Research Datasets: Comprehensive experimental datasets including 952 researchgrade
prompts across prompt length variants (S+M+L) and adaptive benchmarking
scenarios, with seven-dimension rubric scores, token usage metrics, and cost
tracking supporting future cybersecurity AI evaluation research.
Academic Publications: Primary research paper documenting prompt length
effects on cost-quality trade-offs in SOC/GRC tasks and validating adaptive
IFN712 RESEARCH IN IT PRACTICE 7
benchmarking methodologies. Secondary outputs include presentation materials for
Week 12-13 oral examination and conference submission drafts.
Practical Implementation Guidelines: Evidence-based recommendations for
SOC/GRC teams integrating generative AI responsibly, including cost optimization
strategies, bias mitigation best practices, and prompt engineering guidance.
5.2 New Knowledge
Prompt Engineering Science: First empirical quantification of prompt length
effects on quality and cost efficiency in cybersecurity operations, providing
evidence-based guidance for SOC/GRC AI deployment budgets and performance
optimization.
Bias Mitigation Validation: Demonstration of Focus Sentence Prompting
effectiveness in cybersecurity evaluation, extending Domhan & Zhu's findings to
security-critical applications.
Adaptive Benchmarking Methodology: Novel continuous evaluation framework
incorporating live SOC/GRC policies and threat intelligence, addressing static
benchmark staleness while maintaining reproducibility.
5.3 Future Work Timeline
Week 9-10: Experimental Execution - Conduct experimental runs using stratified
sampling with cost-optimized model selection. Generate and evaluate 100 adaptive
prompts against static baselines.
Week 11: Data Analysis - Statistical analysis of prompt length versus cost-quality
relationships with significance testing and comprehensive visualizations.
Week 12-13: Research Completion - Finalize research paper, prepare oral
presentation with platform demonstrations, submit reproducible analysis code and
GitHub repository.
IFN712 RESEARCH IN IT PRACTICE 8
6 References
Catchpoint. (2024). GenAI performance benchmarking. Catchpoint.
https://www.catchpoint.com/learn/gen-ai-benchmark
Domhan, T., & Zhu, D. (2025). Same evaluation, more tokens: On the effect of input
length for machine translation evaluation using large language models. arXiv.
https://arxiv.org/abs/2505.01761
Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T.,
Zhang, R., & Ahmed, N. K. (2024). Bias and fairness in large language models: A
survey. Computational Linguistics, 50(3), 1-79.
https://doi.org/10.1162/coli_a_00524
Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., & Chen, Z. (2024). Token-Budget-Aware
LLM reasoning. Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Findings), 1-18. https://arxiv.org/abs/2412.18547
Hevner, A., March, S. T., Park, J., & Ram, S. (2004). Design science in information
systems research. MIS Quarterly, 28(1), 75-105. https://doi.org/10.2307/25148625
Hong, K., Troynikov, A., Huber, J., & McGuire, M. (2025). Generative benchmarking
(Technical Report). Chroma Research. https://research.trychroma.com/generativebenchmarking
Saad-Falcon, J., Khattab, O., Potts, C., & Zaharia, M. (2024). ARES: An automated
evaluation framework for retrieval-augmented generation systems. Proceedings of
the 2024 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, 4473-4492.
https://doi.org/10.18653/v1/2024.naacl-long.248
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R.,
Santoro, A., Gupta, A., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A.,
Power, A., Ray, A., Warstadt, A., Kocurek, A. W., Safaya, A., Tazarv, A., ... Wu, Z. (2023).
Beyond the imitation game: Quantifying and extrapolating the capabilities of
language models. Transactions on Machine Learning Research.
https://openreview.net/forum?id=uyTL5Bvosj
Vaishnavi, V., & Kuechler, B. (2013). Design science research in information systems.
http://www.desrist.org/design-research-in-information-systems
IFN712 RESEARCH IN IT PRACTICE 9
Wahréus, J., Hussain, A. M., & Papadimitratos, P. (2025). CySecBench: Generative AIbased
cybersecurity-focused prompt dataset for benchmarking large language
models. arXiv. https://arxiv.org/abs/2501.01335
Zhang, C., Côté, M.-A., Albada, M., Sankaran, A., Stokes, J. W., Wang, T., Abdi, A., Blum,
W., & Abdul-Mageed, M. (2025). DefenderBench: A toolkit for evaluating language
agents in cybersecurity environments. arXiv. https://arxiv.org/abs/2506.00739